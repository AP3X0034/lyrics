{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Songwriting and Language Generation using TensorFlow\n",
    "## Writing a sonnet in the style of William Shakespeare using a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based heavily upon the first lab exercise from MIT Deep Learning 6.S191, in which a Recurrent Neural Network (RNN) is used to generate synthetic music based upon Irish folk songs.\n",
    "\n",
    "I have adapted the code to generate Shakespearean Sonnets rather than music, using the entirety of the sonnets of William Shakespeare to train the model.\n",
    "\n",
    "For anyone else looking to learn more about Deep Learning I thoroughly recommend checking out the lectures and exercises from the aforementioned MIT open course at http://introtodeeplearning.com/\n",
    "\n",
    "Sonnets sourced from http://www.shakespeares-sonnets.com/all.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code used under the MIT License.\n",
    "© MIT 6.S191: Introduction to Deep Learning\n",
    "\n",
    "http://introtodeeplearning.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we are using a GPU, if not switch runtimes\n",
    "#   using Runtime > Change Runtime Type > GPU\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to obtain our input data - the complete collection of Shakespeare's sonnets.\n",
    "We can scrape this information using the Beautiful Soup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.shakespeares-sonnets.com/all.php'\n",
    "page = requests.get(url)\n",
    "\n",
    "# create an instance of the BeautifulSoup class, which will parse the html (content) from the requests response\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "alltext = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim to retrieve only the sonnets from the webpage\n",
    "alltext = alltext[alltext.index('All Sonnets'):alltext.index('Copyright')]\n",
    "print(alltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the raw input data, we need to tidy it up a little. For example, each sonnet is labelled by its number in Roman numerals. We don't want to include these Roman numerals in the vocabulary when generating new sonnets, so we must remove them from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = \"[XICVL]+\\.\\n\"\n",
    "alltext = re.sub(regex, \"\", alltext)\n",
    "print(alltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text to lowercase and remove the newline characters, to allow us to count instances of the same word (regardless of their capitalisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext = alltext.replace(\"\\n\",\" \").lower()\n",
    "print(alltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text by whitespace characters to generate a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext = alltext.split(' ')\n",
    "print(alltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cursory glance at our body of text shows that there are instances of characters that will prevent us from creating a vocabulary of unique words.\n",
    "For example, we don't want to distinguish between \"joy\" and \"joy;\", so we should remove the semicolons from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_remove = [\"(\", \")\", \"\\\"\", \"\\'\", \"\", \":\", \";\", \",\", \".\", \"!\", \"?\", \"\\“\", \"\\…\", \"<u+203d>\", \"\\r\", \"\\xa0\", \"-\"]\n",
    "clean_text = list(set(alltext))\n",
    "for character in char_to_remove:\n",
    "    clean_text = [word.replace(character,\"\") for word in clean_text]\n",
    "clean_text = [word for word in clean_text if word != \"\"]\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our input text, let's create a vocabulary of all the unique words in the text. This will be the set of words that our neural network will be able to draw from to create a new sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(clean_text))\n",
    "print(vocab)\n",
    "print(\"There are\", len(vocab), \"unique words in the sonnets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a mapping to represent each unique word in the vocabulary with its own integer value. We also create a reverse mapping to allow us to translate back from id numbers to words. This will allow the neural network to work with numerical representations of the words, rather than entire words themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of words to numbers\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# Reverse the mapping\n",
    "idx2word = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mapping, we can convert phrases of words into vectors of numbers, which will be used to train the model. This is a vital step, because the Embedding layer (which is the first hidden layer of the neural network, a flexible layer which can learn that certain groupings of words appear together more often) requires vectors of numbers as input. The vectorize_string function will also be useful at the end when we will need to vectorize an input seed phrase to the model to generate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to vectorize a given input string of words\n",
    "def vectorize_string(string):\n",
    "  vectorized_words = []\n",
    "  for word in string:\n",
    "      vectorized_words.append(word2idx[word])\n",
    "  vectorized_words = np.array(vectorized_words)\n",
    "  return vectorized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate this in action, let's see what a vectorized representation of the first 10 words of the input text would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_words = vectorize_string(clean_text[:10])\n",
    "print(vectorized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need to break the training data into batches and feed them to the model sequentially.\n",
    "Each [training example? batch?] is comprised of <code>seq_length</code> time steps, each time step being one word input followed by one word output. Setting <code>seq_length</code> to a value greater than 1 therefore allows us to join multiple individual neural networks together sequentially, and due to the fact that we will be using LSTM nodes, which can retain and pass on information, the entire model will be able to learn to predict words based on the context of the <code>seq_length</code> preceding words. [WHAT IS THE BATCH SIZE?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(vectorized_words, seq_length, batch_size):\n",
    "  # the length of the vectorized_words string\n",
    "  n = vectorized_words.shape[0] - 1\n",
    "  # randomly choose the starting indices for the examples in the training batch\n",
    "  idx = np.random.choice(n-seq_length, batch_size)\n",
    "\n",
    "  # construct a list of input sequences for the training batch\n",
    "  input_batch = [vectorized_words[i:i+seq_length] for i in idx]\n",
    "  # construct a list of output sequences for the training batch\n",
    "  output_batch = [vectorized_words[i+1:i+1+seq_length] for i in idx]\n",
    "\n",
    "  # x_batch, y_batch provide the true inputs and targets for network training\n",
    "  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
    "  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
    "\n",
    "  return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1807  134 3135 2894]]\n",
      "[[ 134 3135 2894 3073]]\n",
      "Step   0\n",
      "  input: 1807 ('none')\n",
      "  expected output: 134 ('aprils')\n",
      "Step   1\n",
      "  input: 134 ('aprils')\n",
      "  expected output: 3135 ('word')\n",
      "Step   2\n",
      "  input: 3135 ('word')\n",
      "  expected output: 2894 ('unseen')\n",
      "Step   3\n",
      "  input: 2894 ('unseen')\n",
      "  expected output: 3073 ('why')\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the batching over the timesteps\n",
    "x_batch, y_batch = get_batch(vectorized_words, seq_length=4, batch_size=1)\n",
    "print(x_batch)\n",
    "print(y_batch)\n",
    "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2word[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2word[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write a function that can create a hidden layer of parallel LSTM units.\n",
    "\n",
    "In this notation, <code>rnn_units</code> refers to the number of rnn cells that take up the hidden layer of a given timestep's neural network.\n",
    "\n",
    "You can basically imagine an individual timestep as having its own neural network, where each node in the hidden layer is an LSTM node instead of a basic neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(rnn_units): \n",
    "  return tf.keras.layers.LSTM(\n",
    "    rnn_units, \n",
    "    return_sequences=True, \n",
    "    recurrent_initializer='glorot_uniform',\n",
    "    recurrent_activation='sigmoid',\n",
    "    stateful=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set out the structure of the network itself (i.e. the structure of each of the individual NNs that join together sideways to form the <code>num_steps</code> RNN), which will comprise:<br><br>\n",
    "i) an Embedding layer, <br>\n",
    "[TO CLARIFY - DOES THE EMBEDDING LAYER TAKE A vocab_length DIM VECTOR [0,0,0,0,1,0,0], REPRESENTING A GIVEN TIMESTEP'S WORD, OR DOES IT TAKE IN THE VECTORIZED_WORDS VECTOR (e.g. [213,32,4,492] IF THE seq_length = 4]<br>\n",
    "[TO CLARIFY - WHAT EXACTLY IS EACH LITTLE NN'S INPUT AT A GIVEN TIMESTEP? I BELIEVE IT IS THE LATTER, SINCE THE POINT OF AN EMBEDDING LAYER IS TO ALLOW THE MODEL TO LEARN THAT CERTAIN GROUPS OF WORDS APPEAR TOGETHER (\" You could one-hot encoded all the words but you will lose the notion of similarity between them.\"]\n",
    "which will transform an input vector of dimension <code>vocab_size</code> (as this is a classification exercise: if we had a three-word vocabulary <code>['ONE','TWO','THREE']</code>, the word <code>'TWO'</code> would correspond to an input of <code>[1,0,0]</code>) into a dense vector [CLARIFY THIS LAST PART].<br>\n",
    "(note that one word is passed in per timestep, so at each timestep the size of the input vector <code>x_t</code> will be equal to the vocab size, as each word is represented by a different <code>[0,0,1]</code> vector).<br><br>\n",
    "ii) an LSTM layer, containing rnn_units number of LSTM cells (as explained in the previous cell).<br><br>\n",
    "iii) an Output layer, with the number of nodes being equal to the vocabulary size (as this is a classification exercise: if we had a three-word vocabulary <code>['ONE','TWO','THREE']</code>, the word 'TWO' would correspond to an output of <code>[0,1,0]</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the RNN Model ###\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    # Layer 1: Embedding layer to transform indices into dense vectors \n",
    "    #   of a fixed embedding size\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "\n",
    "    # Layer 2: LSTM with `rnn_units` number of units. \n",
    "    LSTM(rnn_units),\n",
    "\n",
    "    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
    "    #   into the vocabulary size. \n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Build a simple model with default hyperparameters. You will get the \n",
    "#   chance to change these later.\n",
    "model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3196\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           818176    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 3196)          3275900   \n",
      "=================================================================\n",
      "Total params: 9,341,052\n",
      "Trainable params: 9,341,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(vectorized_words, seq_length=100, batch_size=32)\n",
    "pred = model(x)\n",
    "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain predictions from untrained model\n",
    "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode predictions from untrained model, find they're a bit rubbish\n",
    "print(x)\n",
    "print(\"Input: \\n\", repr(\" \".join(idx2word[x[0]])))\n",
    "print()\n",
    "print(\"Next Word Predictions: \\n\", repr(\" \".join(idx2word[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING THE MODEL: Part 1: Defining the loss function ###\n",
    "\n",
    "# define the loss function to compute and return the loss between the true labels and predictions (logits). \n",
    "# Set the argument from_logits=True.\n",
    "def compute_loss(labels, logits):\n",
    "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return loss\n",
    "\n",
    "# compute the loss using the true next characters from the example batch \n",
    "# and the predictions from the untrained model several cells above\n",
    "example_batch_loss = compute_loss(y, pred)\n",
    "\n",
    "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter setting and optimization ###\n",
    "\n",
    "# Optimization parameters:\n",
    "num_training_iterations = 2000  # Increase this to train longer\n",
    "batch_size = 4  # Experiment between 1 and 64\n",
    "seq_length = 100  # Experiment between 50 and 500\n",
    "learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
    "\n",
    "# Model parameters: \n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256 \n",
    "rnn_units = 1024  # Experiment between 1 and 2048\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define optimizer and training operation ###\n",
    "\n",
    "# instantiate a new model for training using the `build_model`\n",
    "# function and the hyperparameters created above.'''\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "# Instantiate an optimizer with its learning rate.\n",
    "#   Checkout the tensorflow website for a list of supported optimizers.\n",
    "#   https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\n",
    "#   Try using the Adam optimizer to start\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y): \n",
    "  # Use tf.GradientTape()\n",
    "  with tf.GradientTape() as tape:\n",
    "  \n",
    "    # Feed the current input into the model and generate predictions\n",
    "    y_hat = model(x)\n",
    "  \n",
    "    # compute the loss\n",
    "    loss = compute_loss(y, y_hat)\n",
    "\n",
    "  # Now, compute the gradients \n",
    "#    complete the function call for gradient computation. \n",
    "#       Remember that we want the gradient of the loss with respect all \n",
    "#       of the model parameters. \n",
    "#       HINT: use `model.trainable_variables` to get a list of all model\n",
    "#       parameters.\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  \n",
    "  # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "history = []\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "pbar = tqdm(range(num_training_iterations))\n",
    "for iter in pbar:\n",
    "\n",
    "  # Grab a batch and propagate it through the network\n",
    "  x_batch, y_batch = get_batch(vectorized_words, seq_length, batch_size)\n",
    "  loss = train_step(x_batch, y_batch)\n",
    "\n",
    "  # Update the progress bar\n",
    "  history.append(loss.numpy().mean())\n",
    "  pbar.set_description(\"loss: {}\".format(loss.numpy().mean()))\n",
    "\n",
    "  # Update the model with the changed weights!\n",
    "  if iter % 100 == 0:     \n",
    "    model.save_weights(checkpoint_prefix)\n",
    "    \n",
    "    \n",
    "# Save the trained model and the weights\n",
    "model.save_weights(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Restore the model weights for the last checkpoint after training\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, generation_length=1000):\n",
    "  # Evaluation step (generating ABC text using the learned RNN model)\n",
    "\n",
    "  # Convert the start string to numbers (vectorize)\n",
    "  input_eval = vectorize_string(start_string)\n",
    "  print(input_eval)\n",
    "  input_eval = [word2idx[word] for word in start_string] \n",
    "  print(input_eval)\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  tqdm._instances.clear()\n",
    "\n",
    "  for i in tqdm(range(generation_length)):\n",
    "      # evaluate the inputs and generate the next word predictions\n",
    "      predictions = model(input_eval)\n",
    "      \n",
    "      # Remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      \n",
    "      # use a multinomial distribution to sample\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # Pass the prediction along with the previous hidden state\n",
    "      #   as the next inputs to the model\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      # add the predicted word to the generated text\n",
    "      text_generated.append(idx2word[predicted_id])\n",
    "    \n",
    "  return ([start_string, text_generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model and the function defined above to generate song lyrics of 100 words\n",
    "# Choose a word that appears in the vocabulary (lower case) to seed the generator\n",
    "generated_text = generate_text(model, start_string=[\"shall\"], generation_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
